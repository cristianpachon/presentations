\documentclass[10pt,compress]{beamer}
%\documentclass[10pt,compress,draft]{beamer}
\mode<presentation>
\usepackage{graphicx}
%\usepackage{beamerthemesplit}
\usepackage[ansinew]{inputenc}
%\usepackage[spanish]{babel}
\usepackage{amsmath,amssymb}
\usepackage{eqlist}
\usepackage{amsfonts} % Simbolos matematicos
\usepackage{amsthm} %Enunciados predefinidos y demostraciones
\usepackage{bbm}
\usepackage{enumerate}
\usepackage[dvips]{epsfig} %Incluir figuras .eps
\usepackage{epsf}%Incluir figuras .eps
\usepackage{eurosym}
\usepackage{subfigure}
\usepackage{graphicx}
\useoutertheme[footline=authortitle]{miniframes}
\useinnertheme{circles}
\usepackage{multirow}
\usepackage{animate}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage[ruled,vlined]{algorithm2e}

\usepackage{chicago}%,e:/latex/psfrag

%%%%%%%%%%
%    1   %
%%%%%%%%%%
\title[Introduction to Deep Learning   \qquad{} \insertframenumber %/54]
%/\inserttotalframenumber]
/\pageref{last_page}]
{\LARGE Introduction to Deep Learning}
\author[Cristian Pach\'on Garc\'ia]{{
   \large Cristian Pach\'on Garc\'ia 
  }
\\
%{\small January 2019}
}
\date{\today}

\begin{document}
\SweaveOpts{concordance=TRUE}

\frame{\titlepage}


\frame{\scriptsize{\tableofcontents} }
\AtBeginSubsection[]
{
  \begin{frame}<beamer>
    \frametitle{}
    \scriptsize{\tableofcontents[currentsection,currentsubsection]}
  \end{frame}
}

\AtBeginSection[]
{
  \begin{frame}<beamer>
   \frametitle{}
    \scriptsize{\tableofcontents[currentsection,currentsubsection]}
  \end{frame}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}


\frame{\frametitle{What is Machine Learning?}
\begin{itemize}
\item Machine learning (ML) is the \textbf{scientific study} of 
\textbf{algorithms} and \textbf{statistical 
models} that computer systems use to perform a \textbf{specific task without 
using explicit instructions}, relying on patterns \textbf{learn from data}.

\item The process of making the machine learn is called 
\textbf{the training process}. 


\end{itemize}
}

\frame{

\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/setosa.png}
%\end{minipage}
\end{figure}

}

\frame{
\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/algorithm.png}
%\end{minipage}
\end{figure}

}

\frame{
\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/setosa_classify.png}
%\end{minipage}
\end{figure}

}

\frame{
ML algorithms can be classified into two different groups:

\begin{itemize}
\item Supervised learning.
\item Unsupervised learning.
\end{itemize}
}


\frame{
Supervised learning is the machine learning task of learning a function 
that \textbf{maps an input to an output}.
\centering
\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/class_reg.jpeg}
}

\frame{
Unsupervised learning is the machine learning task that allows us to discover
patterns from the data. Rather than prediction a variable (temperature, flower 
type, etc.) these algorithms are aimed to discover patterns in the data.
}

\frame{
\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
 & Athens & Barcelona & Brussels & Calais & Cherbourg & $\dotsi$ \\ 
  \hline
Athens & 0 & 3313 & 2963 & 3175 & 3339 & $\dotsi$ \\ 
  Barcelona & 3313 & 0& 1318 & 1326 & 1294 & $\dotsi$ \\ 
  Brussels & 2963 & 1318 & 0 & 204 & 583 & $\dotsi$ \\ 
  Calais & 3175 & 1326 & 204 & 0 & 460 & $\dotsi$ \\ 
  Cherbourg & 3339 & 1294 & 583 & 460 & 0 & $\dotsi$ \\
  $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\ddots$ \\
   \hline
\end{tabular}
\label{european_distances}
\end{table}
}


\frame{
\begin{figure}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/europ_cities.png}
\end{figure}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Linear regression}
\frame{\frametitle{Linear regression}
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & height & weight \\ 
  \hline
1 & 173.37 & 69.76 \\ 
  2 & 174.18 & 71.36 \\ 
  3 & 173.16 & 70.85 \\ 
  4 & 175.60 & 69.91 \\ 
  5 & 174.33 & 67.45 \\ 
  6 & 173.18 & 71.77 \\ 
  7 & 174.49 & 70.46 \\ 
  8 & 174.74 & 70.44 \\ 
  9 & 174.58 & 69.82 \\ 
  10 & 173.69 & 69.99 \\ 
  $\vdots$ & \vdots & \vdots\\ 
  200 & 173.61 & 70.213 \\
   \hline
\end{tabular}
\end{table}
}

\frame{
Is there any relationship between \textit{height} and \textit{weight}?
\centering
\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/h_w_first.png}
}


\frame{
\begin{itemize}
\item We would like to find two coefficients ($w$ and $b$) such that 
$weight = b + w * height$.

\item In general, given two pairs of variables $x$ and $y$, we would like to 
find two coefficients ($w$ and $b$) such that $y = b + w * x$.

\item w is known as the \textit{weight} and b is known as the \textit{bias}.
\end{itemize}
}


\frame{
\begin{center}
\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/h_w_second.png}
\end{center}

How can we find $w$ and $b$ such that $y = b + w * x$ is a "good approximation" 
to the data points?
}

\frame{
More important question than \textit{how} is: \textbf{why} do we know
to know these parameters?

\begin{itemize}
\item If a system is modeled by an equation, it can help to predict what can
happen under certain circumstances.
\end{itemize}
}

\frame{
\begin{itemize}
\item Let's suppose $b=4.057$ and $w = 0.3769$. It means 
$$y=4.057 + 0.3769*x.$$ 
What would be the \textit{weight} ($y$) of someone whose \text{height} ($x$) is
174 cm? $y =  4.057 + 0.3769*174 = 69.6376$ kg.
\end{itemize}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ingredients for Deep Learning}
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Loss function}
\frame{\frametitle{Loss function}
\begin{itemize}
\item Previously, we asked the model to predict the \textit{weight} of a persona
whose \text{height} is 174 cm. 
\item The prediction was 69.6376 kg.
\item \textbf{Notation:} when we use the model to predict, we use a special 
symbol for the results: \textbf{$\hat{y}$}.
\item So, in the previous example: $\hat{y} = 69.6376$
\end{itemize}
}

\frame{
How can we find $w$ and $b$ such that $y = b + w * x$ is a "good approximation" 
to the data points?
\begin{itemize}
\item We need a metric that tells what is "good" and what is "bad".
\item We want a metric that is close to 0 when the model is correct.
\item We want a metric that increases as long as the model is not correct.
\end{itemize}
}

\frame{
Let's assume we have $w$ and $b$. For instance, at random we choose $w=0.2$ and
$b=3$:
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & height(x) & weight(y) & prediction($\hat{y}$) & $error= (y-\hat{y})^2$ \\ 
  \hline
  1 & 173.37 & 69.76 & 37.67 & 1029.39 \\ 
  2 & 174.18 & 71.36 & 37.84 & 1123.97 \\ 
  3 & 173.16 & 70.85 & 37.63 & 1103.53 \\ 
  \vdots & \vdots & \vdots & \vdots & \vdots \\ 
  200 & 173.6189 & 70.31279 & 37.72378 & 1062.0434 \\
   \hline
\end{tabular}
\end{table}

$$loss = MSE = \sqrt{\frac{1}{200}(1029.39 + 1123.97  + \cdots + 1062.0434}) = 31.86411$$
}


\frame{
\begin{itemize}
\item In general, the formula for loss function (for regression problems) 
is the following one:
$$loss = \sqrt{\frac{1}{n}\sum_{i = 1}^n(y_i - \hat{y_i})^2} = \sqrt{\frac{1}{n}\sum_{i = 1}^n\Bigg( y_i - (b + wx_i) \Bigg) ^2}.$$


\item Our goal is to find \textit{b} and \textit{w} such that the loss is 
minimum.

\item How??? Choosing \textit{b} and \textit{w} randomly??? We will use
\textbf{Gradient Descent} algorithm.

\end{itemize}
}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Gradient Descent}
\frame{\frametitle{Gradient Descent}
The  idea is the following one:
\begin{itemize}
\item Given a function that depends on two paramenters $f(b, w)$, we want 
to find $w^*$ and $b^*$ such that $min\{f(b, w)\} = f(b^*, w^*)$.
\item Gradient Descent (GD) allows us to find such parameter.
\item GD works with more than two variables, i.e, let's supose
we want to find the minimum value of a function that depends on $n$ variables
$f(z_1, z_2, \dots, z_n)$, GD allows us to find $z_1^*, z_2^*, \dots, z_n^*$
such that $min\{f(z_1, z2, \dots, z_n)\} = f(z_1^*, z_2^*, \dots, z_n^*)$.

\end{itemize}
}


\frame{
\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/w2.png}
%\end{minipage}
\end{figure}
}



\frame{
\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/gd.png}
%\end{minipage}
\end{figure}
}




\frame{
\begin{algorithm}[H]
\SetAlgoLined
\KwResult{Find $w^*$ such that $min\{f(w)\}=f(w^*)$ }
 $epsilon = 10^{-6}$\;
 $w_0 =$ (random) initial point\;
 $is\_minimum = FALSE$\;
 $learning\_rate = 0.1$\;
 $f_0 = f(w_0)$\;
 \While{not $is\_minimum$}{
  $derivative = f'(w_0)$\;
  $w_1 = w_0 - learning\_rate \cdot derivative$\;
  $f_1 = f(w_1)$\;
  \If{$\mid f_1-f_0\mid < epsilon$}{
   $is\_minimum = TRUE$\;
   }
   $w_0 =w_1$\;
   $f_0 = f_1$\;
 }
 \Return $w_1$
 \caption{Gradient Descent algorithm}
\end{algorithm}

}

\frame{
Given $f(w) = w^2$ and $w_0 = 1$, we want to apply  GD algorithm to obtain the
minimum. We expect to obtain, after some iterartions,  $w^* =0$ (or close to 0).
\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/gd_manual.png}
%\end{minipage}
\end{figure}
}


\frame{
Remember that:
\begin{itemize}
\item $f(w) = w^2$.
\item $derivative = f'(w) = 2w$.
\item $w_1 = w_0 - learning\_rate \cdot derivative$.
\end{itemize}

We choose as learning rate a value of 0.01.

\begin{table}[ht]
\centering
\begin{tabular}{rrrrrrr}
  \hline
 iteration & $w_0$ & $f(w_0)$ & derivative & $w_1$ & $f(w_1)$ & $\mid f(w_1) - f(w_0) \mid$ \\ 
  \hline
  2 & 1.00 & 1.00 & 2.00 & 0.80 & 0.64 & 0.36 \\ 
  3 & 0.80 & 0.64 & 1.60 & 0.64 & 0.41 & 0.23 \\ 
  4 & 0.64 & 0.41 & 1.28 & 0.51 & 0.26 & 0.15 \\ 
  $\vdots$ & $\vdots$ & $\vdots$ &$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\  
  26 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 \\ 
  27 & 0.00 & 0.00 & 0.01 & 0.00 & 0.00 & 0.00 \\ 

   \hline
\end{tabular}
\end{table}
}

\frame{
Back to our initial problem, we want $w$ and $b$ such that minimise
$$\sqrt{\frac{1}{n}\sum_{i = 1}^n\Bigg( weight - (b + w\cdot height) \Bigg) ^2}.$$

\begin{itemize}
\item Applying GD we obtain $b = 4.0570$ and $w = 0.3769$.
\item Loss function is 1.00795.
\item Any other set of parameters would provide a loss function greater than
1.00795.

\end{itemize}
 
}

\frame{
\begin{itemize}
\item GD is an algorithm that helps us find the optimal values for the 
parameters. That is why it is called \textit{optimiser}.
\item There are many \textit{optimisers} algorithms:
\begin{itemize}
\item Momentum
\item RMSprop
\item Adam
\item AdaMax
\item Adadelta
\item ...
\end{itemize}
\end{itemize}
}

% Gradient descent es uno de los algoritmos pero hay mas
% Mencionar el resto de algoritmos

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deep Learning}
%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Introduction}
\frame{\frametitle{Introduction}
\textbf{Why} do we need \textit{another} kind of models? 
\begin{itemize}
\item Unfortunately real life is not linear. We need more 
\textit{complex}/\textit{flexible} models.
\item Neural networks (Deep Learning models) are very flexible models. They
are able to capture very non-linear patterns and model with a high precision.
\end{itemize}
}



\frame{\frametitle{ImageNet problem}
\begin{itemize}
\item The ImageNet project is a large visual database designed for use in 
visual object recognition software research. 
\item More than 14 million images have been hand-annotated.
\item ImageNet contains more than 20.000 categories with a typical category, 
such as "balloon" or "strawberry", consisting of several hundred images.
\end{itemize}
}

\frame{
\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/imagenet.png}
%\end{minipage}
\end{figure}
}


\frame{
\begin{figure}
\centering
%\begin{minipage}{\textwidth}
    \includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/error_imagenet.png}
%\end{minipage}
\end{figure}
}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Perceptron}
\frame{\frametitle{Single neuron model (perceptron)}
\centering
\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/perceptron.png}

\begin{itemize}
\item \textbf{Weights} and \textbf{bias} are the parameters that define the 
behaviour. They must be estimated during training.
\item The \textbf{output} (y) is derived from a sum of the weighted inputs plus a bias 
term.
\item The \textbf{activation} function \textbf{introduces non-linearities}.
\end{itemize}
}

\frame{\frametitle{Single neuron model: Linear Regression}
\centering
\includegraphics[width=\textwidth,height=0.8\textheight,keepaspectratio]{./images/perceptron_lr.png}

}

%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Backpropagation}
\frame{\frametitle{Backpropagation}
Rembember that GD formula is:
$$w = w - learning\_rate \cdot derivative.$$

We are going to use the following notation: 
$$derivative = \frac{\partial L}{\partial w}.$$
Our goal is to find a way to calculate the derivative in an efficient way.
}



%%Multilayer perceptrons - MLP
%%% Deep neural network
%% Why has DL become so famous? Imagenet problem

%%%%%%%%%%%%%%%%%%%%%%%%
\frame{
  \centering \Huge
  \emph{Thank You}
  \label{last_page}
}

\end{document}
